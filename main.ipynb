{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explication des cellules d'initialisation\n",
    "Nous avons testé plusieurs approches : en local, avec Pycharm. Dans le cloud, avec Kaggle & Google Colab.\n",
    "Nous avons donc mis en place plusieurs configurations pour chaque environnement.\n",
    "Si local, vérifier que java 11 est bien installé. Si Kaggle, uploader le csv comme dataset afin de l'avoir à disposition.\n"
   ],
   "id": "b53a79cfe8c543ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Made by :\n",
    "- Maxime Kets\n",
    "- Thomas Blaisot\n",
    "- Cedric Sanchez\n",
    "- Pierre-Louis Guinel"
   ],
   "id": "686c30793d90ff28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from pyspark.sql.functions import col, when, avg, max, min, count, row_number, split, explode, trim\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType"
   ],
   "id": "eb1d7dcfd3b9c9fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ],
   "id": "5b0c778ba43edd7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\"\n",
    "file_save_path = \"./data/saved.parquet\""
   ],
   "id": "707702dc242a6abf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#database configuration\n",
    "path_mysql = \"mysql/mysql-connector-j-9.1.0.jar\"\n",
    "url = \"jdbc:mysql://localhost:3306/openfood_db\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ],
   "id": "9c5f19c057366327"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start_time = time.time()\n",
    "print(\"Démarrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs réduits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.jars\", path_mysql) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Réduction des logs\n",
    "\n",
    "print(\"PySpark chargé\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis échantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV chargé.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # Échantillonnage à 20%\n",
    "    print(\"Echantillonage terminé\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Données sauvegardées au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet chargé.\")\n",
    "\n",
    "    # Vérifier et supprimer l'ancienne table Hive\n",
    "    table_name = \"hive_table\"\n",
    "    hive_table_path = f\"spark-warehouse/{table_name}\"\n",
    "    if table_name in [t.name for t in spark.catalog.listTables(\"default\")]:\n",
    "        print(f\"La table '{table_name}' existe déjà et doit être supprimé.\")\n",
    "        spark.sql(f\"DROP TABLE {table_name}\")\n",
    "        print(f\"Table Hive '{table_name}' supprimée.\")\n",
    "    if os.path.exists(hive_table_path):\n",
    "        shutil.rmtree(hive_table_path)\n",
    "        print(f\"Répertoire associé '{hive_table_path}' supprimé.\")\n",
    "\n",
    "    # Création de la table Hive\n",
    "    print(\"Création et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur rencontrée : {e}\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'exécution : {elapsed_time:.2f} secondes\")\n"
   ],
   "id": "6ae102e100d37e35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse préliminaire\n",
    "### 1. Mise en valeur des lignes, colonnes\n",
    "\n"
   ],
   "id": "40f43791f06e7f75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")\n",
    "hive_row_count = df_hive.count()\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ],
   "id": "b612d03bf4227e74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### .2 Gestion des valeurs manquantes\n",
   "id": "f4c8e354495c933c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate missing data percentage for each column\n",
    "total_rows = df_parquet.count()\n",
    "missing_data = (\n",
    "    df_parquet.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n",
    "        for c in df_parquet.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Transform columns into rows (melt operation)\n",
    "missing_data_melted = missing_data.selectExpr(\n",
    "    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n",
    "        len(df_parquet.columns),\n",
    "        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n",
    "    )\n",
    ").filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n",
    "\n",
    "# Identify columns with 100% missing data\n",
    "columns_to_drop = (\n",
    "    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n",
    "    .select(\"Column\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_cleanedby_missing_value = df_parquet.drop(*columns_to_drop)\n",
    "\n",
    "# Display the top 10 columns with the highest missing percentages\n",
    "print(\"Top 10 columns with the highest missing percentages:\")\n",
    "missing_data_melted.show(10, truncate=False)\n",
    "\n",
    "# Print dropped columns\n",
    "print(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ],
   "id": "f1f2cd5c8027ba70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Gestion des valeurs doublons",
   "id": "f9374118b0ec149f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyzing Duplicates in 'code', 'product_name', and 'brands'\n",
    "duplicates = (\n",
    "    df_cleanedby_missing_value.groupBy(\"code\", \"product_name\", \"brands\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# Affiche le nombre de doublons identifiés\n",
    "print(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\n",
    "duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates where 'code', 'product_name', and 'brands' are the same\n",
    "df_cleaned_by_duplicate = df_cleanedby_missing_value.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n",
    "\n",
    "print(f\"Number of rows before removing duplicates: {df_cleanedby_missing_value.count()}\")\n",
    "print(f\"Number of rows after removing duplicates: {df_cleaned_by_duplicate.count()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ],
   "id": "8281a564bcf968f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Gestion des valeurs aberrantes",
   "id": "2222a7e8144037a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Extraire les valeurs numériques de la colonne \"quantity\"\n",
    "df_cleaned_by_outliers = df_cleaned_by_duplicate.withColumn(\n",
    "    \"quantity_numeric\",\n",
    "    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Détecter les colonnes numériques\n",
    "numeric_columns = [\n",
    "    field.name for field in df_cleaned_by_outliers.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))\n",
    "]\n",
    "print(f\"Numeric columns detected: {numeric_columns}\")\n",
    "\n",
    "if not numeric_columns:\n",
    "    print(\"No numeric columns found. Please check your data.\")\n",
    "else:\n",
    "    total_rows_before = df_cleaned_by_outliers.count()\n",
    "    removed_rows_total = 0\n",
    "\n",
    "    # Boucle sur les colonnes numériques pour détecter les outliers\n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            # Calcul des quartiles avec approxQuantile\n",
    "            quantiles = df_cleaned_by_outliers.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "            if len(quantiles) < 2:\n",
    "                print(f\"Column '{column}' has insufficient data. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Filtrer les outliers\n",
    "            df_outliers = df_cleaned_by_outliers.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "            removed_rows = df_outliers.count()\n",
    "            removed_rows_total += removed_rows\n",
    "\n",
    "            print(f\"Column '{column}': {removed_rows} rows detected as outliers.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{column}': {e}\")\n",
    "\n",
    "    print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "    print(f\"Total rows removed as outliers: {removed_rows_total}\")\n",
    "    print(f\"Total rows remaining: {total_rows_before - removed_rows_total}\")"
   ],
   "id": "3bcb1052b18adf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data cleaning\n",
   "id": "410388dfc4f22f11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_parquet = df_outliers\n",
    "df_parquet.describe()"
   ],
   "id": "c2aca905bd254022"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "    \"sodium_100g\",\n",
    "    \"sugars_100g\",\n",
    "    \"fiber_100g\"\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ],
   "id": "ffa11748d680571c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert the columns to the appropriate format\n",
    "numeric_cols = [\n",
    "    'nutriscore_score', 'energy-kcal_100g', 'fat_100g', 'saturated-fat_100g',\n",
    "    'proteins_100g', 'sugars_100g', 'salt_100g', 'fiber_100g'\n",
    "]\n",
    "# apply the conversion\n",
    "for column in numeric_cols:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ],
   "id": "8310b78251960fef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformation des données Transform :\n",
    "Ajouter des colonnes calculées, par exemple : Indice de qualité nutritionnelle \n",
    "Calculer un score basé sur les nutriments (e.g., sodium, sugar, fiber). \n",
    "Extraire la catégorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \n",
    "Regrouper les données par catégories (categories) pour analyser les tendances (e.g., moyenne des calories par catégorie).\n",
    "\n",
    "--> Quel calcules effectuer ?  \n",
    "--> Quel catégories créer ?\n"
   ],
   "id": "4a0e5bc54797adf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ],
   "id": "732463fda688fbe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ],
   "id": "48a517b7d7bec1b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ],
   "id": "ae58228cce031577"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# add a new column 'main_category' by extracting the first category from 'categories'\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df_transformed = df_transformed.withColumn(\"main_category\", split(col(\"categories\"), \",\").getItem(0))\n",
    "df_transformed.show(5, truncate=False)\n"
   ],
   "id": "ce4af611d2902877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create a new column with quality nutrition score (IQN)\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"nutrition_score\",\n",
    "    0.3 * col(\"fiber_100g\") +\n",
    "    0.2 * col(\"proteins_100g\") -\n",
    "    0.4 * col(\"sugars_100g\") -\n",
    "    0.3 * col(\"saturated-fat_100g\") -\n",
    "    0.1 * col(\"salt_100g\")\n",
    ")\n",
    "df_transformed.show(5, truncate=False)\n"
   ],
   "id": "87e24e92e89132c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse exploratoire :\n",
    "Utiliser des fonctions de calcul sur fenêtre pour : \n",
    "Trouver les produits les plus caloriques par catégorie. \n",
    "Identifier les tendances de production par brands (marques). \n",
    "Générer des statistiques descriptives (e.g., médiane, moyenne des nutriments par catégorie"
   ],
   "id": "d81f162352e10019"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Étape : Suppression des doublons après explosion ---\n",
    "df_unique_products = df_transformed.dropDuplicates(['code'])\n",
    "\n",
    "# --- Étape 2 : Catégoriser les produits en fonction du score nutritionnel ---\n",
    "\n",
    "# Ajouter une catégorie de score nutritionnel\n",
    "df_unique_products = df_unique_products.withColumn(\n",
    "    \"score_category\",\n",
    "    when(col(\"nutrition_score\") > 0, \"Positif\")\n",
    "    .when(col(\"nutrition_score\") == 0, \"Neutre\")\n",
    "    .otherwise(\"Négatif\")\n",
    ")\n",
    "\n",
    "# Compter les produits par catégorie de score\n",
    "score_category_counts = df_unique_products.groupBy(\"score_category\").count()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"Distribution des catégories de score nutritionnel :\")\n",
    "score_category_counts.show()\n",
    "\n",
    "# --- Étape 3 : Identifier les produits les plus sains et les moins sains ---\n",
    "\n",
    "# Inclure des informations supplémentaires pour plus de contexte\n",
    "top_healthy = df_unique_products.orderBy(col(\"nutrition_score\").desc()).select(\n",
    "    \"product_name\", \"brands\", \"main_category\", \"nutrition_score\"\n",
    ").limit(10)\n",
    "\n",
    "print(\"Top 10 des produits les plus sains :\")\n",
    "top_healthy.show(truncate=False)\n",
    "\n",
    "top_unhealthy = df_unique_products.orderBy(col(\"nutrition_score\").asc()).select(\n",
    "    \"product_name\", \"brands\", \"main_category\", \"nutrition_score\"\n",
    ").limit(10)\n",
    "\n",
    "print(\"Top 10 des produits les moins sains :\")\n",
    "top_unhealthy.show(truncate=False)\n",
    "\n",
    "# --- Étape 4 : Produits les plus caloriques par catégorie principale ---\n",
    "\n",
    "# Utiliser 'main_category' pour le groupement\n",
    "window_spec = Window.partitionBy(\"main_category\").orderBy(col(\"energy-kcal_100g\").desc())\n",
    "\n",
    "# Ajouter une colonne avec le rang des calories\n",
    "df_unique_products = df_unique_products.withColumn(\"calorie_rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filtrer pour obtenir les produits les plus caloriques par catégorie principale\n",
    "most_caloric_products = df_unique_products.filter(col(\"calorie_rank\") == 1).select(\n",
    "    \"main_category\", \"product_name\", \"brands\", \"energy-kcal_100g\"\n",
    ")\n",
    "\n",
    "print(\"Produits les plus caloriques par catégorie principale :\")\n",
    "most_caloric_products.show(truncate=False)\n",
    "\n",
    "# --- Étape 5 : Analyse des tendances des marques ---\n",
    "\n",
    "# Compter le nombre de produits par marque\n",
    "brand_trends = df_unique_products.groupBy(\"brands\").agg(\n",
    "    count(\"*\").alias(\"product_count\")\n",
    ").orderBy(col(\"product_count\").desc())\n",
    "\n",
    "print(\"Tendances de production par marques (Top 10) :\")\n",
    "brand_trends.limit(10).show(truncate=False)\n",
    "\n",
    "# --- Étape 6 : Statistiques descriptives par catégorie principale ---\n",
    "\n",
    "# Calculer les statistiques descriptives pour chaque catégorie principale\n",
    "category_statistics = df_unique_products.groupBy(\"main_category\").agg(\n",
    "    avg(\"energy-kcal_100g\").alias(\"avg_calories\"),\n",
    "    avg(\"fat_100g\").alias(\"avg_fat\"),\n",
    "    avg(\"proteins_100g\").alias(\"avg_proteins\"),\n",
    "    avg(\"sugars_100g\").alias(\"avg_sugars\"),\n",
    "    avg(\"salt_100g\").alias(\"avg_salt\")\n",
    ").orderBy(\"main_category\")\n",
    "\n",
    "print(\"Statistiques descriptives par catégorie principale :\")\n",
    "category_statistics.show(truncate=False)\n",
    "\n",
    "# --- Étape 7 : Analyses supplémentaires ---\n",
    "\n",
    "# Produits les plus salés\n",
    "top_salty = df_unique_products.orderBy(col(\"salt_100g\").desc()).select(\n",
    "    \"product_name\", \"brands\", \"main_category\", \"salt_100g\"\n",
    ").limit(10)\n",
    "\n",
    "print(\"Top 10 des produits les plus salés :\")\n",
    "top_salty.show(truncate=False)\n",
    "\n",
    "# Produits les plus sucrés\n",
    "top_sugary = df_unique_products.orderBy(col(\"sugars_100g\").desc()).select(\n",
    "    \"product_name\", \"brands\", \"main_category\", \"sugars_100g\"\n",
    ").limit(10)\n",
    "\n",
    "print(\"Top 10 des produits les plus sucrés :\")\n",
    "top_sugary.show(truncate=False)\n",
    "\n",
    "# Distribution du score nutritionnel par catégorie principale\n",
    "category_score_distribution = df_unique_products.groupBy(\"main_category\").agg(\n",
    "    avg(\"nutrition_score\").alias(\"avg_nutrition_score\"),\n",
    "    min(\"nutrition_score\").alias(\"min_nutrition_score\"),\n",
    "    max(\"nutrition_score\").alias(\"max_nutrition_score\")\n",
    ").orderBy(\"main_category\")\n",
    "\n",
    "print(\"Distribution du score nutritionnel par catégorie principale :\")\n",
    "category_score_distribution.show(truncate=False)"
   ],
   "id": "2b093af0d97e30a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde des données Save :\n",
    "Partitionner les données par catégories (categories) et années (year). \n",
    "Sauvegarder les résultats transformés en format Parquet avec compression Snappy. \n",
    "Sauvegarder les résultats transformés dans les bases de données: postgresql/sqlserver/mysql/Snowflake/BigQuery"
   ],
   "id": "4b1df41c69463168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the transformed data to Docker database\n",
    "df_transformed.write.jdbc(url=url, table=\"nom_de_la_table\", mode=\"append\", properties=properties)\n"
   ],
   "id": "d37d62ff2f2444e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#saving data with snappy\n",
    "df_transformed.write \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(file_save_path)\n",
    "print(\"Data saved in Parquet format with Snappy compression.\")"
   ],
   "id": "c5e5a2e45a0e24f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Présentation des résultats :\n",
    "Visualiser les résultats sous forme de graphiques ou tableaux \n",
    "(les étudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab "
   ],
   "id": "3136d534aaac8216"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert to pandas for visualization\n",
    "df_unique_products = df_transformed.dropDuplicates(['code'])\n",
    "pd_transformed = df_unique_products.toPandas()\n"
   ],
   "id": "b9393b452e29747f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pd_transformed['nutrition_score'].dropna(), bins=30, kde=True)\n",
    "plt.title('Distribution du Score Nutritionnel')\n",
    "plt.xlabel('Score Nutritionnel')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.show()\n"
   ],
   "id": "62bb77dc6e6d3c44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sélectionner les top produits les plus caloriques\n",
    "top_caloric_products = pd_transformed.sort_values('energy-kcal_100g', ascending=False).head(10)\n",
    "print(\"Top 10 des Produits les Plus Caloriques :\")\n",
    "print(top_caloric_products[['product_name', 'energy-kcal_100g']])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='product_name', y='energy-kcal_100g', data=top_caloric_products)\n",
    "plt.title('Top 10 des Produits les Plus Caloriques')\n",
    "plt.xlabel('Produit')\n",
    "plt.ylabel('Énergie (kcal/100g)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ],
   "id": "d22e37c6a72b1dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_brands = pd_transformed['brands'].value_counts().head(10).reset_index()\n",
    "top_brands.columns = ['brands', 'product_count']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='brands', y='product_count', data=top_brands)\n",
    "plt.title('Top 10 des Marques par Nombre de Produits')\n",
    "plt.xlabel('Marque')\n",
    "plt.ylabel('Nombre de Produits')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ],
   "id": "3da9149a310654a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculer les moyennes\n",
    "nutrient_means = pd_transformed.groupby('main_category').agg({\n",
    "    'energy-kcal_100g': 'mean',\n",
    "    'fat_100g': 'mean',\n",
    "    'proteins_100g': 'mean',\n",
    "    'sugars_100g': 'mean',\n",
    "    'salt_100g': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Visualiser la moyenne des calories par catégorie\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.barplot(x='main_category', y='energy-kcal_100g', data=nutrient_means)\n",
    "plt.title('Moyenne des Calories par Catégorie Principale')\n",
    "plt.xlabel('Catégorie Principale')\n",
    "plt.ylabel('Énergie Moyenne (kcal/100g)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ],
   "id": "2962c1a053151d2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = px.box(pd_transformed, x='main_category', y='nutrition_score',\n",
    "             title='Score Nutritionnel par Catégorie Principale')\n",
    "fig.update_layout(xaxis_title='Catégorie Principale', yaxis_title='Score Nutritionnel')\n",
    "fig.show()\n"
   ],
   "id": "984edf9b15a21d2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#stopper la session\n",
    "spark.stop()"
   ],
   "id": "ab07bf1711d6b48b"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
